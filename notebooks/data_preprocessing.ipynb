{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Advanced Project\\\\Lesion-Stroke-Segmentation\\\\notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Advanced Project\\\\Lesion-Stroke-Segmentation'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Steps\n",
    "* Re-size Images\n",
    "* Correct Orientation\n",
    "* Normalization\n",
    "* Image Augmentation\n",
    "    - Image Random Rotation\n",
    "    - Image Random Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataProcessingEntity:\n",
    "    training_data: Path\n",
    "    testing_data: Path\n",
    "    train_csv: Path\n",
    "    test_csv: Path\n",
    "    preprocess_train_in: Path\n",
    "    preprocess_train_op: Path\n",
    "    preprocess_test_in: Path\n",
    "    img_height: int\n",
    "    img_width: int\n",
    "    slice_stride: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lesionSeg.constant import *\n",
    "from lesionSeg.Utils.common import read_yaml, create_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, params = PARAMS_FILE_PATH, config = CONFIG_FILE_PATH):\n",
    "        self.params = read_yaml(params)\n",
    "        self.config = read_yaml(config)\n",
    "\n",
    "        create_directory([self.config.artifact_root])\n",
    "\n",
    "    def data_preprocessing_config(self) -> DataProcessingEntity:\n",
    "        params = self.params.image_preprocess\n",
    "        config = self.config.data_preprocessing\n",
    "\n",
    "        create_directory([config.root_dir, \n",
    "                          config.preprocess_train_in, \n",
    "                          config.preprocess_train_op, \n",
    "                          config.preprocess_test_in])\n",
    "\n",
    "        data_preprocessing_entity = DataProcessingEntity(\n",
    "            training_data = Path(config.training_data),\n",
    "            train_csv = Path(config.train_csv),\n",
    "            test_csv = Path(config.test_csv),\n",
    "            testing_data = Path(config.testing_data),\n",
    "            preprocess_train_in = Path(config.preprocess_train_in),\n",
    "            preprocess_train_op = Path(config.preprocess_train_op),\n",
    "            preprocess_test_in = Path(config.preprocess_test_in),\n",
    "            img_height = params.img_height,\n",
    "            img_width = params.img_width,\n",
    "            slice_stride = params.slice_stride\n",
    "        )\n",
    "        \n",
    "        return data_preprocessing_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from bids import BIDSLayout\n",
    "from lesionSeg.logging import logger\n",
    "from lesionSeg.Exception.exception import CustomeException\n",
    "from scipy import ndimage\n",
    "from typing import Dict, Tuple\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"nibabel\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, config: DataProcessingEntity):\n",
    "        self.config = config\n",
    "\n",
    "    def calculate_resize_factors(self, slice_data: np.ndarray) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Calculate resize factors for the given slice data.\n",
    "        \"\"\"\n",
    "        current_width, current_height = slice_data.shape[:2]\n",
    "        DESIRED_HEIGHT = self.config.img_height\n",
    "        DESIRED_WIDTH = self.config.img_width\n",
    "        height_factor = DESIRED_HEIGHT / current_height\n",
    "        width_factor = DESIRED_WIDTH / current_width\n",
    "        return height_factor, width_factor\n",
    "\n",
    "    def train_df_process(self, df) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process the training dataframe by selecting the necessary columns and merging input and mask files.\n",
    "        \"\"\"\n",
    "        selected_cols = [\"path\", \"subject\"]\n",
    "        df_input = df[df['suffix'] == 'T1w'][selected_cols].rename(columns={'path': 'og_input_path'})\n",
    "        df_mask = df[df['suffix'] == 'mask'][selected_cols].rename(columns={'path': 'og_mask_path'})\n",
    "        df_merge = df_input.merge(df_mask, on='subject', how='inner').reset_index(drop=True)\n",
    "        return df_merge[['og_input_path', 'og_mask_path', 'subject']]\n",
    "\n",
    "    def test_df_process(self, df) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process the testing dataframe by selecting the necessary columns.\n",
    "        \"\"\"\n",
    "        selected_cols = [\"path\", \"subject\"]\n",
    "        df_input = df[df['suffix'] == 'T1w'][selected_cols].rename(columns={'path': 'og_input_path'})\n",
    "        return df_input\n",
    "\n",
    "    def _process_slice(self, slice_data: np.ndarray, output_path: str, is_mask: bool = False):\n",
    "        \"\"\"\n",
    "        Helper method to process and save a single slice.\n",
    "        \"\"\"\n",
    "        height_factor, width_factor = self.calculate_resize_factors(slice_data)\n",
    "        resized_slice = ndimage.zoom(slice_data, (width_factor, height_factor, 1), order=1)\n",
    "        \n",
    "        if is_mask:\n",
    "            resized_slice = (resized_slice * 255).astype(np.uint8)  # Ensure binary mask is scaled correctly\n",
    "        cv2.imwrite(output_path, resized_slice)\n",
    "        \n",
    "\n",
    "    def preprocess_train_dataset(self, n: Dict):\n",
    "        \"\"\"\n",
    "        Preprocess one training subject.\n",
    "        \"\"\"\n",
    "        X_path = n[\"og_input_path\"]\n",
    "        y_path = n[\"og_mask_path\"]\n",
    "        subject = n['subject']\n",
    "        SLICE_STRIDE = self.config.slice_stride\n",
    "\n",
    "        in_nib = nb.load(X_path)\n",
    "        mask_nib = nb.load(y_path)\n",
    "\n",
    "        X_img = nb.as_closest_canonical(in_nib).get_fdata()\n",
    "        y_img = nb.as_closest_canonical(mask_nib).get_fdata()\n",
    "\n",
    "        img_no = X_img.shape[2] // SLICE_STRIDE\n",
    "\n",
    "        for i in range(0, img_no, SLICE_STRIDE):\n",
    "            input_slice = X_img[:, :, i:i+3]  # Stack 3 slices\n",
    "            mask_slice = np.expand_dims(y_img[:, :, i+2], axis=-1)  # Use the middle slice for mask\n",
    "\n",
    "            input_path = os.path.join(self.config.preprocess_train_in, f\"{subject}_{i}.png\")\n",
    "            mask_path = os.path.join(self.config.preprocess_train_op, f\"{subject}_{i}.png\")\n",
    "\n",
    "            self._process_slice(input_slice, input_path)\n",
    "            self._process_slice(mask_slice, mask_path, is_mask=True)\n",
    "\n",
    "    def preprocess_test_dataset(self, n: Dict):\n",
    "        \"\"\"\n",
    "        Preprocess one test subject.\n",
    "        \"\"\"\n",
    "        X_path = n[\"og_input_path\"]\n",
    "        subject = n['subject']\n",
    "        SLICE_STRIDE = self.config.slice_stride\n",
    "\n",
    "        X_img = nb.as_closest_canonical(nb.load(X_path)).get_fdata()\n",
    "        img_no = X_img.shape[2] // SLICE_STRIDE\n",
    "\n",
    "        for i in range(0, img_no, SLICE_STRIDE):\n",
    "            input_slice = X_img[:, :, i]\n",
    "            input_path = os.path.join(self.config.preprocess_test_in, f\"{subject}_{i}.png\")\n",
    "            self._process_slice(input_slice, input_path)\n",
    "\n",
    "    def preprocess_dataset(self):\n",
    "        \"\"\"\n",
    "        Process the entire dataset.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            train_layout = BIDSLayout(root=self.config.training_data, validate=False)\n",
    "            test_layout = BIDSLayout(root=self.config.testing_data, validate=False)\n",
    "\n",
    "            train_df = self.train_df_process(train_layout.to_df())\n",
    "            test_df = self.test_df_process(test_layout.to_df())\n",
    "\n",
    "            logger.info(\"Training and Testing DataFrames have been created and pre-processed.\")\n",
    "\n",
    "            train_df.apply(lambda x: self.preprocess_train_dataset(x), axis=1)\n",
    "            test_df.apply(lambda x: self.preprocess_test_dataset(x), axis=1)\n",
    "\n",
    "            logger.info(f\"Pre-processed training data saved at: {self.config.preprocess_train_in}\")\n",
    "            logger.info(f\"Pre-processed testing data saved at: {self.config.preprocess_test_in}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomeException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-16 02:55:43,871]: INFO: common : Read YAML File: params.yaml\n",
      "[2025-02-16 02:55:43,877]: INFO: common : Read YAML File: config\\config.yaml\n",
      "[2025-02-16 02:55:43,878]: INFO: common : Directory has been Created: artifact\n",
      "[2025-02-16 02:55:43,880]: INFO: common : Directory has been Created: artifact\\data_preprocessed\n",
      "[2025-02-16 02:55:43,891]: INFO: common : Directory has been Created: artifact\\data_preprocessed\\Training\\input\n",
      "[2025-02-16 02:55:43,894]: INFO: common : Directory has been Created: artifact\\data_preprocessed\\Training\\output\n",
      "[2025-02-16 02:55:43,894]: INFO: common : Directory has been Created: artifact\\data_preprocessed\\Testing\\input\n",
      "[2025-02-16 02:55:54,334]: INFO: 4160121676 : Training and Testing DataFrames have been created and pre-processed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_processing_config = config.data_preprocessing_config()\n",
    "    data_preprocessing = DataPreprocessing(data_processing_config)\n",
    "    data_preprocessing.preprocess_dataset()\n",
    "except Exception as e:\n",
    "    e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
